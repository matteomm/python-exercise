{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Taurus', 'Virgo', 'Capricorn']\n",
      "['Aries', 'Leo', 'Sagittarius']\n"
     ]
    }
   ],
   "source": [
    "zodiac_elements = {\"water\": [\"Cancer\", \"Scorpio\", \"Pisces\"], \"fire\": [\"Aries\", \"Leo\", \"Sagittarius\"], \"earth\": [\"Taurus\", \"Virgo\", \"Capricorn\"], \"air\":[\"Gemini\", \"Libra\", \"Aquarius\"]}\n",
    "\n",
    "print(zodiac_elements['earth'])\n",
    "print(zodiac_elements['fire'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an invalid key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a Zodiac element\n"
     ]
    }
   ],
   "source": [
    "zodiac_elements = {\"water\": [\"Cancer\", \"Scorpio\", \"Pisces\"], \"fire\": [\"Aries\", \"Leo\", \"Sagittarius\"], \"earth\": [\"Taurus\", \"Virgo\", \"Capricorn\"], \"air\":[\"Gemini\", \"Libra\", \"Aquarius\"], \"energy\":\"Not a Zodiac element\"}\n",
    "\n",
    "print(zodiac_elements[\"energy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try/Except to Get a Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "caffeine_level = {\"espresso\": 64, \"chai\": 40, \"decaf\": 0, \"drip\": 120}\n",
    "\n",
    "caffeine_level['matcha'] = 30\n",
    "\n",
    "try:\n",
    "  print(caffeine_level['matcha'])\n",
    "except KeyError:\n",
    "  print(\"Unknown Caffeine Level\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safely Get a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100019\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "user_ids = {\"teraCoder\": 100019, \"pythonGuy\": 182921, \"samTheJavaMaam\": 123112, \"lyleLoop\": 102931, \"keysmithKeith\": 129384}\n",
    "\n",
    "tc_id = user_ids.get('teraCoder',100000)\n",
    "print(tc_id)\n",
    "\n",
    "stack_id = user_ids.get ('superStackSmash', 100000)\n",
    "print(stack_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def mult_two_add_three(number):\n",
    "  \n",
    "  print(number*2 + 3)\n",
    "  \n",
    "# Call mult_two_add_three() here:\n",
    "mult_two_add_three(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def mult_x_add_y(number, x, y):\n",
    "  print(number*x + y)\n",
    "\n",
    "mult_x_add_y (5,2,3)\n",
    "mult_x_add_y (1,3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a spreadsheet called Downloads with 0 rows\n",
      "Create a spreadsheet called Applications with 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Define create_spreadsheet():\n",
    "def create_spreadsheet(title, row_count):\n",
    "  print(\"Create a spreadsheet called \"+title+ ' with '+ str(row_count) + ' rows')\n",
    "\n",
    "  \n",
    "# Call create_spreadsheet() below with the required arguments:\n",
    "create_spreadsheet(\"Downloads\", 0)\n",
    "create_spreadsheet('Applications', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 56 years old and my dad is 96 years old\n"
     ]
    }
   ],
   "source": [
    "def calculate_age(current_year, birth_year):\n",
    "  age = current_year - birth_year\n",
    "  return age\n",
    "  \n",
    "my_age = calculate_age(2049, 1993)\n",
    "dads_age = calculate_age(2049, 1953)\n",
    "print(\"I am \"+str(my_age)+\" years old and my dad is \"+str(dads_age)+\" years old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = ['twin bed', 'twin bed', 'headboard', 'queen bed', 'king bed', 'dresser', 'dresser', 'table', 'table', 'nightstand', 'nightstand', 'king bed', 'king bed', 'twin bed', 'twin bed', 'sheets', 'sheets', 'pillow', 'pillow']\n",
    "\n",
    "inventory_len = len(inventory)\n",
    "first = inventory [0]\n",
    "last = inventory[-1]\n",
    "inventory_2_6 = inventory [2:6]\n",
    "first_3 = inventory[:3]\n",
    "twin_beds = inventory.count('twin bed')\n",
    "inventory.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting elements in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "votes = ['Jake', 'Jake', 'Laurie', 'Laurie', 'Laurie', 'Jake', 'Jake', 'Jake', 'Laurie', 'Cassie', 'Cassie', 'Jake', 'Jake', 'Cassie', 'Laurie', 'Cassie', 'Jake', 'Jake', 'Cassie', 'Laurie']\n",
    "\n",
    "jake_votes = votes.count('Jake')\n",
    "print(jake_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poem Afterimages was published by Audre Lorde in 1997\n",
      "The poem The Shadow was published by William Carlos Williams in 1915\n",
      "The poem Ecstasy was published by Gabriela Mistral in 1925\n",
      "The poem Georgia Dusk was published by Jean Toomer in 1923\n",
      "The poem Parting Before Daybreak was published by An Qi in 2014\n",
      "The poem The Untold Want was published by Walt Whitman in 1871\n",
      "The poem Mr. Grumpledump's Song was published by Shel Silverstein in 2004\n",
      "The poem Angel Sound Mexico City was published by Carmen Boullosa in 2013\n",
      "The poem In Love was published by Kamala Suraiyya in 1965\n",
      "The poem Dream Variations was published by Langston Hughes in 1994\n",
      "The poem Dreamwood was published by Adrienne Rich in 1987\n"
     ]
    }
   ],
   "source": [
    "highlighted_poems = \"Afterimages:Audre Lorde:1997,  The Shadow:William Carlos Williams:1915, Ecstasy:Gabriela Mistral:1925,   Georgia Dusk:Jean Toomer:1923,   Parting Before Daybreak:An Qi:2014, The Untold Want:Walt Whitman:1871, Mr. Grumpledump's Song:Shel Silverstein:2004, Angel Sound Mexico City:Carmen Boullosa:2013, In Love:Kamala Suraiyya:1965, Dream Variations:Langston Hughes:1994, Dreamwood:Adrienne Rich:1987\"\n",
    "\n",
    "# print(highlighted_poems)\n",
    "\n",
    "highlighted_poems_list = highlighted_poems.split(',')\n",
    "\n",
    "# print(highlighted_poems_list)\n",
    "\n",
    "highlighted_poems_stripped = []\n",
    "\n",
    "for poem in highlighted_poems_list:\n",
    "  highlighted_poems_stripped.append(poem.strip())\n",
    "  \n",
    "# print(highlighted_poems_stripped)\n",
    "\n",
    "highlighted_poems_details = []\n",
    "\n",
    "for poem in highlighted_poems_stripped:\n",
    "  highlighted_poems_details.append(poem.split(':'))\n",
    "  \n",
    "titles = []\n",
    "poets = []\n",
    "dates = []\n",
    "\n",
    "for poem in highlighted_poems_details:\n",
    "  titles.append(poem[0])\n",
    "  poets.append(poem[1])\n",
    "  dates.append(poem[2])\n",
    "  \n",
    "for i in range(0,len(highlighted_poems_details)):\n",
    "    print('The poem {} was published by {} in {}'.format(titles[i], poets[i], dates[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poem My Beard by Shel Silverstein was originally published in Where the Sidewalk Ends in 1974.\n"
     ]
    }
   ],
   "source": [
    "def poem_description(publishing_date, author, title, original_work):\n",
    "  poem_desc = \"The poem {title} by {author} was originally published in {original_work} in {publishing_date}.\".format(publishing_date=publishing_date, author = author, title = title, original_work = original_work)\n",
    "  return poem_desc\n",
    "\n",
    "my_beard_description = poem_description('1974', 'Shel Silverstein', 'My Beard', 'Where the Sidewalk Ends')\n",
    "print(my_beard_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods with Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Circle:\n",
    "  pi = 3.14\n",
    "  \n",
    "  def area(self, radius):\n",
    "    return Circle.pi * radius ** 2\n",
    "  \n",
    "circle = Circle()\n",
    "pizza_area = circle.area(12 / 2)\n",
    "teaching_table_area = circle.area(36 / 2)\n",
    "round_room_area = circle.area(11460 / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New circle with diameter: 36\n"
     ]
    }
   ],
   "source": [
    "class Circle:\n",
    "  pi = 3.14\n",
    "  \n",
    "  # Add constructor here:\n",
    "  def __init__(self, diameter):\n",
    "    print(\"New circle with diameter: {diameter}\".format(diameter=diameter))\n",
    "    \n",
    "teaching_table = Circle(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'part_of_speech'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e50881b13852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# grabbing a part of speech function:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpart_of_speech\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_part_of_speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'part_of_speech'"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Pyshics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.77777777777778\n",
      "32.0\n",
      "226800\n",
      "The GE train supplies 226800 Newtons of force.\n",
      "90000000000000000\n",
      " A 1kg bomb supplies 90000000000000000 Joules\n",
      "226800\n",
      "22680000\n",
      "The GE train does 22680000 Joules of work over 100 meters.\n"
     ]
    }
   ],
   "source": [
    "train_mass = 22680\n",
    "train_acceleration = 10\n",
    "train_distance = 100\n",
    "\n",
    "bomb_mass = 1\n",
    "\n",
    "def f_to_c(f_temp):\n",
    "  c_temp = (f_temp - 32) * 5/9\n",
    "  print (c_temp)\n",
    "  return c_temp\n",
    "\n",
    "\n",
    "f100_in_celsius = f_to_c (100)\n",
    "\n",
    "def c_to_f (c_temp):\n",
    "  f_temp = (c_temp * (9/5)) + 32\n",
    "  print (f_temp)\n",
    "  return f_temp\n",
    "\n",
    "c0_in_fahrenheit = c_to_f (0)\n",
    "\n",
    "def get_force (mass, acceleration):\n",
    "  result = mass * acceleration\n",
    "  print (result)\n",
    "  return result\n",
    "\n",
    "train_force=get_force(train_mass,train_acceleration)\n",
    "\n",
    "print ('The GE train supplies ' + str(train_force) + ' '+ 'Newtons of force.' )\n",
    "\n",
    "def get_energy (mass, c=3*10**8):\n",
    "  return mass*c**2\n",
    "\n",
    "bomb_energy = get_energy(bomb_mass)\n",
    "print (bomb_energy)\n",
    "print (' A 1kg bomb supplies '+ str(bomb_energy)+ ' ' + 'Joules')\n",
    "\n",
    "def get_work (mass, acceleration, distance):\n",
    "  force = get_force (mass,acceleration)\n",
    "  return force* distance\n",
    "\n",
    "train_work = get_work(22680, 10, 100)\n",
    "print (train_work)\n",
    "\n",
    "print (\"The GE train does \" + str(train_work) +  \" Joules of work over \" + str (train_distance) +  \" meters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-51e4256556da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# define vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# load model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# define vectors\n",
    "summer_vec = nlp(\"summer\").vector\n",
    "winter_vec = nlp(\"winter\").vector\n",
    "\n",
    "# compare similarity\n",
    "print(f\"The cosine distance between the word embeddings for 'summer' and 'winter' is: {cosine(summer_vec, winter_vec)}\\n\")\n",
    "\n",
    "# define vectors\n",
    "mustard_vec = nlp(\"mustard\").vector\n",
    "amazing_vec = nlp(\"amazing\").vector\n",
    "\n",
    "# compare similarity\n",
    "print(f\"The cosine distance between the word embeddings for 'mustard' and 'amazing' is: {cosine(mustard_vec, amazing_vec)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1f6a3e5d33f7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1f6a3e5d33f7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    rom nltk import RegexpParser\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rom nltk import RegexpParser\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "from np_chunk_counter import np_chunk_counter\n",
    "\n",
    "# define noun-phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for-loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common np-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compiling and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='Dorothy'>\n",
      "Dorothy\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# compile your regular expression here\n",
    "regular_expression = re.compile(\"\\w{7}\")\n",
    "\n",
    "# check for a match to character_1 here\n",
    "result_1 = regular_expression.match(character_1)\n",
    "print(result_1)\n",
    "\n",
    "# store and print the matched text here\n",
    "match_1 = result_1.group(0)\n",
    "print(match_1)\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
    "result_2 = re.match(\"\\w{7}\",character_2)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching and finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'the_wizard_of_oz_text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9702aa5c4215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import L. Frank Baum's The Wonderful Wizard of Oz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moz_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the_wizard_of_oz_text.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# search oz_text for an occurrence of 'wizard' here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the_wizard_of_oz_text.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# search oz_text for an occurrence of 'wizard' here\n",
    "found_wizard = re.search(\"wizard\",oz_text)\n",
    "print(found_wizard)\n",
    "\n",
    "# find all the occurrences of 'lion' in oz_text here\n",
    "all_lions = re.findall(\"lion\",oz_text)\n",
    "print(all_lions)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_lions = len(all_lions)\n",
    "print(number_lions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
